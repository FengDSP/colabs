{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.mean=1.0913210588254132\n",
      "loss.mean=0.9850030766634523\n",
      "loss.mean=0.8883022642920164\n",
      "loss.mean=0.8007055281144693\n",
      "loss.mean=0.7216600587492563\n",
      "loss.mean=0.650583007706536\n",
      "loss.mean=0.5868734915700964\n",
      "loss.mean=0.5299253517707319\n",
      "loss.mean=0.47913926430831877\n",
      "loss.mean=0.4339332494308596\n",
      "loss.mean=0.3937511249071102\n",
      "loss.mean=0.35806881772524507\n",
      "loss.mean=0.3263986606318467\n",
      "loss.mean=0.2982918936929117\n",
      "loss.mean=0.27333962548098123\n",
      "loss.mean=0.2511725214961169\n",
      "loss.mean=0.23145949142293545\n",
      "loss.mean=0.2139056404773614\n",
      "loss.mean=0.19824973042811564\n",
      "loss.mean=0.1842613637665792\n",
      "loss.mean=0.17173806450426127\n",
      "loss.mean=0.16050238702408812\n",
      "loss.mean=0.15039914523624553\n",
      "loss.mean=0.14129282103771834\n",
      "loss.mean=0.13306518490335079\n",
      "loss.mean=0.12561314212803096\n",
      "loss.mean=0.11884680482198451\n",
      "loss.mean=0.11268778100065878\n",
      "loss.mean=0.10706766681870515\n",
      "loss.mean=0.10192672517350193\n",
      "loss.mean=0.0972127327705962\n",
      "loss.mean=0.0928799777284441\n",
      "loss.mean=0.0888883904939257\n",
      "loss.mean=0.08520279195733799\n",
      "loss.mean=0.08179224399881851\n",
      "loss.mean=0.07862948913346851\n",
      "loss.mean=0.07569046736056892\n",
      "loss.mean=0.07295389970567259\n",
      "loss.mean=0.0704009292379776\n",
      "loss.mean=0.068014811530126\n",
      "loss.mean=0.06578064759541713\n",
      "loss.mean=0.063685153287919\n",
      "loss.mean=0.06171645998857314\n",
      "loss.mean=0.059863942132611134\n",
      "loss.mean=0.05811806776954728\n",
      "loss.mean=0.05647026889646439\n",
      "loss.mean=0.05491282877805467\n",
      "loss.mean=0.05343878387228539\n",
      "loss.mean=0.05204183832733832\n",
      "loss.mean=0.05071628931152849\n",
      "loss.mean=0.049456961690316686\n",
      "loss.mean=0.04825915077952532\n",
      "loss.mean=0.04711857208689871\n",
      "loss.mean=0.04603131710993974\n",
      "loss.mean=0.04499381439057153\n",
      "loss.mean=0.04400279514010115\n",
      "loss.mean=0.04305526284417972\n",
      "loss.mean=0.04214846633949059\n",
      "loss.mean=0.041279875923908665\n",
      "loss.mean=0.040447162121680515\n",
      "loss.mean=0.03964817677632291\n",
      "loss.mean=0.038880936187732654\n",
      "loss.mean=0.03814360604754876\n",
      "loss.mean=0.037434487959043\n",
      "loss.mean=0.03675200735552809\n",
      "loss.mean=0.03609470265513208\n",
      "loss.mean=0.035461215510359086\n",
      "loss.mean=0.03485028202862085\n",
      "loss.mean=0.034260724855285106\n",
      "loss.mean=0.033691446024093144\n",
      "loss.mean=0.03314142049134036\n",
      "loss.mean=0.03260969028024099\n",
      "loss.mean=0.032095359170623505\n",
      "loss.mean=0.03159758787670592\n",
      "loss.mean=0.03111558966233631\n",
      "loss.mean=0.030648626348884055\n",
      "loss.mean=0.03019600467604458\n",
      "loss.mean=0.029757072980271717\n",
      "loss.mean=0.0293312181594602\n",
      "loss.mean=0.028917862895936883\n",
      "loss.mean=0.028516463112844895\n",
      "loss.mean=0.028126505641673082\n",
      "loss.mean=0.027747506081038972\n",
      "loss.mean=0.02737900682891638\n",
      "loss.mean=0.027020575272343247\n",
      "loss.mean=0.026671802120280804\n",
      "loss.mean=0.02633229986674678\n",
      "loss.mean=0.02600170137263607\n",
      "loss.mean=0.02567965855579095\n",
      "loss.mean=0.02536584117990681\n",
      "loss.mean=0.025059935733772982\n",
      "loss.mean=0.024761644393164084\n",
      "loss.mean=0.02447068405842808\n",
      "loss.mean=0.024186785461470187\n",
      "loss.mean=0.023909692336418688\n",
      "loss.mean=0.023639160648784765\n",
      "loss.mean=0.023374957878400843\n",
      "loss.mean=0.023116862351848188\n",
      "loss.mean=0.02286466262046673\n",
      "loss.mean=0.022618156880386327\n",
      "loss.mean=0.022377152431329757\n",
      "loss.mean=0.022141465171219718\n",
      "loss.mean=0.02191091912387679\n",
      "loss.mean=0.021685345997325918\n",
      "loss.mean=0.02146458477043779\n",
      "loss.mean=0.021248481305821177\n",
      "loss.mean=0.021036887987054397\n",
      "loss.mean=0.020829663378500115\n",
      "loss.mean=0.02062667190609058\n",
      "loss.mean=0.020427783557599062\n",
      "loss.mean=0.0202328736010321\n",
      "loss.mean=0.020041822319884082\n",
      "loss.mean=0.0198545147640945\n",
      "loss.mean=0.019670840515637434\n",
      "loss.mean=0.01949069346775536\n",
      "loss.mean=0.01931397161692419\n",
      "loss.mean=0.01914057686670527\n",
      "loss.mean=0.0189704148427031\n",
      "loss.mean=0.0188033947179055\n",
      "loss.mean=0.018639429047735794\n",
      "loss.mean=0.018478433614195475\n",
      "loss.mean=0.018320327278520607\n",
      "loss.mean=0.018165031841816615\n",
      "loss.mean=0.01801247191317371\n",
      "loss.mean=0.017862574784800474\n",
      "loss.mean=0.017715270313745642\n",
      "loss.mean=0.01757049080980698\n",
      "loss.mean=0.017428170929254666\n",
      "loss.mean=0.017288247574021413\n",
      "loss.mean=0.0171506597960349\n",
      "loss.mean=0.01701534870639029\n",
      "loss.mean=0.016882257389080287\n",
      "loss.mean=0.016751330819018858\n",
      "loss.mean=0.016622515784112293\n",
      "loss.mean=0.01649576081114666\n",
      "loss.mean=0.0163710160952764\n",
      "loss.mean=0.01624823343291164\n",
      "loss.mean=0.016127366157815473\n",
      "loss.mean=0.016008369080233575\n",
      "loss.mean=0.015891198428890353\n",
      "loss.mean=0.015775811795695392\n",
      "loss.mean=0.015662168083014197\n",
      "loss.mean=0.01555022745336554\n",
      "loss.mean=0.015439951281416633\n",
      "loss.mean=0.015331302108154674\n",
      "loss.mean=0.015224243597120862\n",
      "loss.mean=0.015118740492599377\n",
      "loss.mean=0.015014758579660668\n",
      "loss.mean=0.014912264645963614\n",
      "loss.mean=0.014811226445227366\n",
      "loss.mean=0.01471161266228829\n",
      "loss.mean=0.014613392879662323\n",
      "loss.mean=0.01451653754553812\n",
      "loss.mean=0.014421017943129682\n",
      "loss.mean=0.01432680616132209\n",
      "loss.mean=0.014233875066546865\n",
      "loss.mean=0.014142198275827515\n",
      "loss.mean=0.014051750130938945\n",
      "loss.mean=0.013962505673627413\n",
      "loss.mean=0.013874440621840485\n",
      "loss.mean=0.013787531346919702\n",
      "loss.mean=0.013701754851710319\n",
      "loss.mean=0.01361708874954595\n",
      "loss.mean=0.013533511244067138\n",
      "loss.mean=0.013451001109835827\n",
      "loss.mean=0.013369537673709323\n",
      "loss.mean=0.01328910079693919\n",
      "loss.mean=0.013209670857962413\n",
      "loss.mean=0.013131228735853982\n",
      "loss.mean=0.013053755794411126\n",
      "loss.mean=0.01297723386684153\n",
      "loss.mean=0.012901645241028886\n",
      "loss.mean=0.012826972645350465\n",
      "loss.mean=0.012753199235022883\n",
      "loss.mean=0.012680308578953123\n",
      "loss.mean=0.012608284647073236\n",
      "loss.mean=0.012537111798137998\n",
      "loss.mean=0.012466774767966078\n",
      "loss.mean=0.012397258658105726\n",
      "loss.mean=0.012328548924907581\n",
      "loss.mean=0.012260631368987293\n",
      "loss.mean=0.012193492125062062\n",
      "loss.mean=0.012127117652145612\n",
      "loss.mean=0.012061494724086881\n",
      "loss.mean=0.01199661042043868\n",
      "loss.mean=0.011932452117642578\n",
      "loss.mean=0.011869007480517762\n",
      "loss.mean=0.011806264454041338\n",
      "loss.mean=0.011744211255408659\n",
      "loss.mean=0.011682836366362661\n",
      "loss.mean=0.011622128525781438\n",
      "loss.mean=0.01156207672251421\n",
      "loss.mean=0.01150267018845581\n",
      "loss.mean=0.011443898391850608\n",
      "loss.mean=0.011385751030816902\n",
      "loss.mean=0.01132821802708362\n",
      "loss.mean=0.011271289519930923\n",
      "loss.mean=0.01121495586032721\n",
      "loss.mean=0.011159207605255102\n",
      "loss.mean=0.011104035512219286\n",
      "[0.98048067 0.98592891 0.99069797 0.99184871 0.98607036 0.97856968\n",
      " 0.99692573 0.99597054 0.99301404 0.9904138  0.9919623  0.9752055\n",
      " 0.97813025 0.97961392 0.99756602 0.9947811  0.99291988 0.99343144\n",
      " 0.98158407 0.99745408 0.9951072  0.98593411 0.97624286 0.98638626\n",
      " 0.99717316 0.98323518 0.99246928 0.98989868 0.99336735 0.99346051\n",
      " 0.9954341  0.99786125 0.97588463 0.9799935  0.99364524 0.99495925\n",
      " 0.99349347 0.98375616 0.99220153 0.98023018 0.99403331 0.99384938\n",
      " 0.99374224 0.9890538  0.99268058 0.99632715 0.9765743  0.97020995\n",
      " 0.9873287  0.99262917 0.98701681 0.98411872 0.96507585 0.99637165\n",
      " 0.99035053 0.99171736 0.99716938 0.9939172  0.99073624 0.99511413\n",
      " 0.98636127 0.99409118 0.99548565 0.97018112 0.98925053 0.99561282\n",
      " 0.98492665 0.98805445 0.98896885 0.99212849 0.99288419 0.99110647\n",
      " 0.97469357 0.98364079 0.99312803 0.99651456 0.993509   0.99379138\n",
      " 0.98926884 0.9857632  0.99482989 0.99683134 0.97897125 0.99620633\n",
      " 0.99173294 0.99627432 0.97773159 0.99474943 0.99383178 0.99148593\n",
      " 0.99703007 0.97880884 0.97139782 0.98204491 0.99406977 0.99578389\n",
      " 0.98902462 0.99644725 0.99402033 0.99302551 0.00880695 0.014393\n",
      " 0.00359963 0.00753044 0.0118282  0.00348392 0.00183118 0.02597425\n",
      " 0.00492732 0.01018404 0.0280436  0.00889559 0.01597989 0.00336177\n",
      " 0.00184    0.00452519 0.00788076 0.00418043 0.00830316 0.01147814\n",
      " 0.00842602 0.00806914 0.02351119 0.00402453 0.0214407  0.00832726\n",
      " 0.01831852 0.01253206 0.0265298  0.00501019 0.01374823 0.01352027\n",
      " 0.02215751 0.01165586 0.01274976 0.00566062 0.00544893 0.02057933\n",
      " 0.00435687 0.0116486  0.02201447 0.01725121 0.01126316 0.00487753\n",
      " 0.03994335 0.01341319 0.01300171 0.00695406 0.02058116 0.01286227\n",
      " 0.00824913 0.0233851  0.01007281 0.00872804 0.0036791  0.01199055\n",
      " 0.00396356 0.01159695 0.01479828 0.01036773 0.00656469 0.00544869\n",
      " 0.00447608 0.01266972 0.01820059 0.00253036 0.00691681 0.00739477\n",
      " 0.00273752 0.0336061  0.00957807 0.01243676 0.00306161 0.0219081\n",
      " 0.00169643 0.01602276 0.01219557 0.0054221  0.00455967 0.01489412\n",
      " 0.00702701 0.00571866 0.00969157 0.00571769 0.01822103 0.01450091\n",
      " 0.00953579 0.0083892  0.00925182 0.00853366 0.0209022  0.00164867\n",
      " 0.00527163 0.00610388 0.00330411 0.01244827 0.0157491  0.01137623\n",
      " 0.01510292 0.00466948]\n"
     ]
    }
   ],
   "source": [
    "# Linear Binary Classification\n",
    "# Given a set of N points in the D-dimensional space and their labels of boolean values, find a hyperplane that separates the true and false class. Return a classifier that takes in a point vector and returns probility.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def binary_classifier_from(X, Y):\n",
    "    '''\n",
    "        X, a float np.array of shape [N, D]\n",
    "        Y, a boolean array of labels, shape [N]\n",
    "    '''\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    D = X.shape[1]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(D)  # D,\n",
    "    b = 0  # 1\n",
    "\n",
    "    lr = 0.001\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return (1 + np.exp(-x)) ** -1\n",
    "    def d_sigmoid(x):\n",
    "        # -1 * (1 + np.exp(-x)) ** -2 * np.exp(-x) * -1\n",
    "        # return sigmoid(x) ** 2 * np.exp(-x)  \n",
    "        return  sigmoid(x) * (1 -  sigmoid(x))\n",
    "    \n",
    "    m = X.mean(0)\n",
    "    std = X.std(0)\n",
    "\n",
    "    def forward(vectors):\n",
    "        standardized_v = (vectors - m) / (std + epsilon)\n",
    "        logit = standardized_v @ W + b  # N\n",
    "        prob = sigmoid(logit)  # N\n",
    "        return prob\n",
    "\n",
    "    for i in range(200):\n",
    "        # Forward\n",
    "        prob = forward(X)  # N\n",
    "        labels = Y * 1.0\n",
    "        loss = - labels * np.log(prob + epsilon) - (1 - labels) * np.log(1 - prob + epsilon)\n",
    "\n",
    "        # Backward\n",
    "        standardized_x = (X - m) / (std + epsilon)\n",
    "        # d_prob = - labels / (prob + epsilon) + (1 - labels) / (1 - prob + epsilon)  # N\n",
    "        # d_logit = prob * (1 - prob) * d_prob # N\n",
    "        d_logit = prob - labels\n",
    "        d_W = standardized_x.T @ d_logit  # D\n",
    "        d_b = d_logit.sum()\n",
    "\n",
    "        W -= lr * d_W\n",
    "        b -= lr * d_b\n",
    "\n",
    "        print(f\"loss.mean={loss.mean()}\")\n",
    "        # break\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "# train_data = np.array([\n",
    "#     [0, 1], [1, 2], [3, 4], [1, 1], [2, 3],\n",
    "# ], dtype=float)\n",
    "# train_labels = np.array([True, True, True, False, True])\n",
    "\n",
    "# train_data = np.array([\n",
    "#     [0, 1], [1, 2],\n",
    "# ], dtype=float)\n",
    "# train_labels = np.array([True, False])\n",
    "\n",
    "train_data = np.concatenate(\n",
    "    [\n",
    "        np.random.randn(100, 10) + np.array([50, 20] + [0] * 8, dtype=float),\n",
    "        np.random.randn(100, 10) + np.array([20, 0] + [0] * 8, dtype=float),\n",
    "    ], axis=0)\n",
    "train_labels = np.array([True] * 100 + [False] * 100)\n",
    "\n",
    "model = binary_classifier_from(train_data, train_labels)\n",
    "print(model(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_logits=[[  0.4   0.   -4.   -1. ]\n",
      " [  0.4   0.   -4.  -10. ]]\n",
      "softmax(test_logits)=[[5.18351093e-01 3.47461129e-01 6.36397256e-03 1.27823806e-01]\n",
      " [5.94308491e-01 3.98376895e-01 7.29652735e-03 1.80862831e-05]]\n",
      "logits_grad=[[ 0.25917555 -0.32626944  0.00318199  0.0639119 ]\n",
      " [ 0.29715425  0.19918845  0.00364826 -0.49999096]]\n",
      "test_logits_w_d=[[ 4.e-01  0.e+00 -4.e+00 -1.e+00]\n",
      " [ 4.e-01  1.e-04 -4.e+00 -1.e+01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1991944394497125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-class logistic regression\n",
    "\n",
    "# loss = numpy.F.cross_entropy(logits, Y)\n",
    "# what is d_logits?\n",
    "\n",
    "# probs = softmax(logits)\n",
    "# loss = -np.log(probs[y]).mean()\n",
    "\n",
    "# loss = - log(e ** logits[y] / SUM_j(e ** logits[j]) )\n",
    "\n",
    "# dL / d(logits[i]) = - probs[y] ** -1 * d (e ** logits[y] / SUM_j(e ** logits[j]) / d(logits[i])\n",
    "# if i != y\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1  * (-1) e ** logits[y] / SUM_j(e ** logits[j]) ** 2 * e ** logits[i]\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1  * (-1) prob[y] * prob[i]\n",
    "#  dL / d(logits[i]) = prob[i]\n",
    "# if i == y\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 * \n",
    "#      (e ** logits[i] * SUM_j(e ** logits[j]) - d(SUM_j(e ** logits[j]))/dlogit[i] * e ** logits[i])\n",
    "#      / SUM_j(e ** logits[j]) ** 2\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 *\n",
    "#      (e ** logits[i] * SUM_j(e ** logits[j]) - e ** logits[i] * e ** logits[i])\n",
    "#      / SUM_j(e ** logits[j]) ** 2\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 * e ** logits[i] *(SUM_j(e ** logits[j]) -  e ** logits[i])\n",
    "#      / SUM_j(e ** logits[j]) ** 2\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 * prob[i] *(1 -  e ** logits[i] / SUM_j(e ** logits[j]) )\n",
    "#  dL / d(logits[i]) = - (1 - prob[i])\n",
    "#  dL / d(logits[i]) = prob[i] - 1\n",
    "\n",
    "def softmax(x, dim):\n",
    "  x_maxes = np.max(x, dim, keepdims=True)\n",
    "  norm_x = x - x_maxes # subtract max for numerical stability\n",
    "  counts = np.exp(norm_x)\n",
    "  counts_sum = counts.sum(dim, keepdims=True)\n",
    "  counts_sum_inv = counts_sum**-1\n",
    "  return counts * counts_sum_inv  # B, V\n",
    "\n",
    "def grad_of_loss_wrt_logits(logits, Y):\n",
    "  d_logits = softmax(logits, 1)\n",
    "  d_logits[range(logits.shape[0]), Y] -= 1.0\n",
    "  return d_logits / logits.shape[0]\n",
    "\n",
    "# softmax(np.zeros([2, 2], dtype=float), 1)\n",
    "\n",
    "test_logits = np.array(\n",
    "  [\n",
    "    [0.4, 0.0, -4.0, -1.0],\n",
    "    [0.4, 0.0, -4.0, -10.0],\n",
    "  ],\n",
    "  dtype=float,\n",
    ")\n",
    "# test_logits.shape\n",
    "test_Y = np.array([1, 3], dtype=int)\n",
    "\n",
    "logits_grad = grad_of_loss_wrt_logits(test_logits, test_Y)\n",
    "print(f\"test_logits={test_logits}\")\n",
    "print(f\"softmax(test_logits)={softmax(test_logits, 1)}\")\n",
    "print(f\"logits_grad={logits_grad}\")\n",
    "\n",
    "# verify by numerical gradient\n",
    "epsilon = 0.0001\n",
    "test_logits_w_d = test_logits.copy()\n",
    "test_logits_w_d[1, 1] += epsilon\n",
    "print(f\"test_logits_w_d={test_logits_w_d}\")\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "  '''\n",
    "    prob: dtype float, shape of [B, C], the probability of each class\n",
    "    labels: dtype int, shape of [B, 1], the class label indices\n",
    "  '''\n",
    "  prob = softmax(logits, 1)\n",
    "  return -np.log(prob)[range(logits.shape[0]), labels].mean()\n",
    "\n",
    "(cross_entropy_loss(test_logits_w_d, test_Y) - cross_entropy_loss(test_logits, test_Y)) / epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
