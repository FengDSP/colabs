{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.mean=0.4791393152765643\n",
      "loss.mean=0.41838117379848255\n",
      "loss.mean=0.369285921512877\n",
      "loss.mean=0.3291588509102313\n",
      "loss.mean=0.2959808169654386\n",
      "loss.mean=0.26824010047622726\n",
      "loss.mean=0.2447996430785635\n",
      "loss.mean=0.22479755231432375\n",
      "loss.mean=0.20757435199533358\n",
      "loss.mean=0.19262030681387834\n",
      "loss.mean=0.17953733421769708\n",
      "loss.mean=0.168011366352509\n",
      "loss.mean=0.15779216630407647\n",
      "loss.mean=0.14867846961230213\n",
      "loss.mean=0.1405069487524031\n",
      "loss.mean=0.13314394104834334\n",
      "loss.mean=0.1264791901999657\n",
      "loss.mean=0.12042106771970801\n",
      "loss.mean=0.1148928916741768\n",
      "loss.mean=0.10983006626290144\n",
      "loss.mean=0.10517784079121043\n",
      "loss.mean=0.10088954001269132\n",
      "loss.mean=0.09692515614889809\n",
      "loss.mean=0.093250220622694\n",
      "loss.mean=0.08983489376341539\n",
      "loss.mean=0.086653225608543\n",
      "loss.mean=0.08368255194290793\n",
      "loss.mean=0.08090299794309395\n",
      "loss.mean=0.07829706798435392\n",
      "loss.mean=0.0758493048583452\n",
      "loss.mean=0.0735460052301705\n",
      "loss.mean=0.07137498091413487\n",
      "loss.mean=0.06932535767514425\n",
      "loss.mean=0.06738740491836097\n",
      "loss.mean=0.06555239092599158\n",
      "loss.mean=0.0638124593208057\n",
      "loss.mean=0.06216052324420401\n",
      "loss.mean=0.06059017438002384\n",
      "loss.mean=0.05909560447003714\n",
      "loss.mean=0.057671537380995896\n",
      "loss.mean=0.056313170117449365\n",
      "loss.mean=0.055016121445909276\n",
      "loss.mean=0.05377638701711349\n",
      "loss.mean=0.0525903000541713\n",
      "loss.mean=0.051454496823153534\n",
      "loss.mean=0.050365886225439936\n",
      "loss.mean=0.04932162295278952\n",
      "loss.mean=0.0483190837305908\n",
      "loss.mean=0.04735584624521688\n",
      "loss.mean=0.04642967041038554\n",
      "loss.mean=0.045538481676938414\n",
      "loss.mean=0.04468035613215772\n",
      "loss.mean=0.04385350716996976\n",
      "loss.mean=0.043056273543236774\n",
      "loss.mean=0.042287108634703106\n",
      "loss.mean=0.04154457080477402\n",
      "loss.mean=0.040827314692769366\n",
      "loss.mean=0.040134083364108314\n",
      "loss.mean=0.03946370120946121\n",
      "loss.mean=0.038815067513592844\n",
      "loss.mean=0.038187150621706085\n",
      "loss.mean=0.03757898263981495\n",
      "loss.mean=0.03698965461323463\n",
      "loss.mean=0.03641831213383909\n",
      "loss.mean=0.035864151332450446\n",
      "loss.mean=0.03532641521770652\n",
      "loss.mean=0.03480439032710605\n",
      "loss.mean=0.03429740365974347\n",
      "loss.mean=0.033804819863587784\n",
      "loss.mean=0.03332603865309809\n",
      "loss.mean=0.03286049243555456\n",
      "loss.mean=0.032407644126764217\n",
      "loss.mean=0.03196698513881462\n",
      "loss.mean=0.031538033524331134\n",
      "loss.mean=0.031120332263272345\n",
      "loss.mean=0.030713447679699735\n",
      "loss.mean=0.03031696797720322\n",
      "loss.mean=0.029930501882773554\n",
      "loss.mean=0.029553677389900314\n",
      "loss.mean=0.029186140592557273\n",
      "loss.mean=0.028827554602525646\n",
      "loss.mean=0.02847759854321128\n",
      "loss.mean=0.028135966613745525\n",
      "loss.mean=0.027802367217726786\n",
      "loss.mean=0.027476522151470593\n",
      "loss.mean=0.027158165847094656\n",
      "loss.mean=0.02684704466617958\n",
      "loss.mean=0.02654291624011826\n",
      "loss.mean=0.0262455488536038\n",
      "loss.mean=0.02595472086801072\n",
      "loss.mean=0.0256702201816986\n",
      "loss.mean=0.025391843724517478\n",
      "loss.mean=0.025119396984020215\n",
      "loss.mean=0.02485269356109221\n",
      "loss.mean=0.024591554752895792\n",
      "loss.mean=0.024335809161195456\n",
      "loss.mean=0.024085292324285534\n",
      "loss.mean=0.023839846370881865\n",
      "loss.mean=0.023599319694467536\n",
      "loss.mean=0.023363566646700293\n",
      "loss.mean=0.02313244724859577\n",
      "loss.mean=0.022905826918299\n",
      "loss.mean=0.022683576214346038\n",
      "loss.mean=0.022465570593399802\n",
      "loss.mean=0.02225169018151935\n",
      "loss.mean=0.022041819558090987\n",
      "loss.mean=0.02183584755161307\n",
      "loss.mean=0.021633667046584502\n",
      "loss.mean=0.02143517480080064\n",
      "loss.mean=0.02124027127240965\n",
      "loss.mean=0.02104886045612792\n",
      "loss.mean=0.020860849728054857\n",
      "loss.mean=0.02067614969856602\n",
      "loss.mean=0.020494674072799625\n",
      "loss.mean=0.020316339518283755\n",
      "loss.mean=0.02014106553928285\n",
      "loss.mean=0.019968774357469403\n",
      "loss.mean=0.019799390798553654\n",
      "loss.mean=0.019632842184527682\n",
      "loss.mean=0.0194690582312029\n",
      "loss.mean=0.01930797095074084\n",
      "loss.mean=0.019149514558896012\n",
      "loss.mean=0.018993625386707968\n",
      "loss.mean=0.01884024179639605\n",
      "loss.mean=0.018689304101225702\n",
      "loss.mean=0.018540754489129858\n",
      "loss.mean=0.01839453694988197\n",
      "loss.mean=0.018250597205630114\n",
      "loss.mean=0.018108882644612613\n",
      "loss.mean=0.01796934225788717\n",
      "loss.mean=0.01783192657891494\n",
      "loss.mean=0.017696587625850774\n",
      "loss.mean=0.017563278846399577\n",
      "loss.mean=0.01743195506510669\n",
      "loss.mean=0.017302572432958477\n",
      "loss.mean=0.017175088379175685\n",
      "loss.mean=0.017049461565089595\n",
      "loss.mean=0.01692565183999663\n",
      "loss.mean=0.016803620198893678\n",
      "loss.mean=0.01668332874200106\n",
      "loss.mean=0.016564740635985998\n",
      "loss.mean=0.016447820076804034\n",
      "loss.mean=0.01633253225407987\n",
      "loss.mean=0.016218843316954463\n",
      "loss.mean=0.01610672034132816\n",
      "loss.mean=0.015996131298433972\n",
      "loss.mean=0.01588704502467853\n",
      "loss.mean=0.015779431192691637\n",
      "loss.mean=0.015673260283528143\n",
      "loss.mean=0.015568503559969386\n",
      "loss.mean=0.015465133040873656\n",
      "loss.mean=0.01536312147652805\n",
      "loss.mean=0.015262442324956542\n",
      "loss.mean=0.0151630697291412\n",
      "loss.mean=0.015064978495116019\n",
      "loss.mean=0.014968144070894423\n",
      "loss.mean=0.014872542526193945\n",
      "loss.mean=0.014778150532923163\n",
      "loss.mean=0.014684945346397438\n",
      "loss.mean=0.014592904787252343\n",
      "loss.mean=0.01450200722402459\n",
      "loss.mean=0.014412231556371778\n",
      "loss.mean=0.014323557198904179\n",
      "loss.mean=0.014235964065602458\n",
      "loss.mean=0.014149432554796827\n",
      "loss.mean=0.014063943534684204\n",
      "loss.mean=0.013979478329361077\n",
      "loss.mean=0.01389601870535079\n",
      "loss.mean=0.013813546858605004\n",
      "loss.mean=0.013732045401959905\n",
      "loss.mean=0.01365149735302893\n",
      "loss.mean=0.013571886122514245\n",
      "loss.mean=0.013493195502920274\n",
      "loss.mean=0.013415409657653168\n",
      "loss.mean=0.013338513110491142\n",
      "loss.mean=0.01326249073541085\n",
      "loss.mean=0.013187327746755864\n",
      "loss.mean=0.013113009689734062\n",
      "loss.mean=0.013039522431231158\n",
      "loss.mean=0.012966852150927992\n",
      "loss.mean=0.012894985332710197\n",
      "loss.mean=0.012823908756358946\n",
      "loss.mean=0.01275360948951215\n",
      "loss.mean=0.012684074879885925\n",
      "loss.mean=0.012615292547746548\n",
      "loss.mean=0.012547250378623533\n",
      "loss.mean=0.012479936516254946\n",
      "loss.mean=0.012413339355756328\n",
      "loss.mean=0.012347447537004928\n",
      "loss.mean=0.01228224993823165\n",
      "loss.mean=0.012217735669812785\n",
      "loss.mean=0.012153894068254645\n",
      "loss.mean=0.012090714690363888\n",
      "loss.mean=0.01202818730759716\n",
      "loss.mean=0.011966301900583278\n",
      "loss.mean=0.011905048653812351\n",
      "loss.mean=0.011844417950485442\n",
      "loss.mean=0.011784400367519434\n",
      "loss.mean=0.011724986670701714\n",
      "loss.mean=0.011666167809989187\n",
      "[0.98527986 0.98111901 0.98765405 0.98527257 0.97869323 0.99325759\n",
      " 0.98860791 0.98960424 0.97917041 0.9832821  0.9892379  0.98783163\n",
      " 0.98990855 0.98420504 0.98740869 0.99447864 0.99147345 0.98049705\n",
      " 0.98872133 0.9906717  0.9901581  0.99311469 0.99116919 0.98871161\n",
      " 0.99147744 0.99257449 0.9889098  0.98999176 0.99210967 0.99050702\n",
      " 0.98729549 0.99193103 0.98832605 0.99382223 0.977879   0.98853222\n",
      " 0.9917964  0.98923959 0.99269984 0.99134892 0.98452079 0.99274463\n",
      " 0.98870886 0.98939373 0.99247206 0.98953636 0.98801203 0.98242783\n",
      " 0.98952835 0.98751665 0.98367096 0.99186524 0.98882635 0.98076872\n",
      " 0.97142013 0.9894052  0.98856828 0.98939335 0.99191086 0.98918827\n",
      " 0.99109499 0.98694358 0.99155852 0.99527778 0.98879906 0.97621346\n",
      " 0.98862605 0.99016441 0.97493718 0.9924156  0.99234574 0.99320972\n",
      " 0.98954119 0.99287736 0.9878306  0.98900975 0.99261915 0.98693913\n",
      " 0.98983488 0.99186238 0.98504576 0.98705782 0.99196821 0.98793833\n",
      " 0.98626415 0.98841004 0.99093015 0.99289465 0.98499141 0.98649175\n",
      " 0.99578824 0.99041571 0.99384608 0.99178166 0.98535046 0.99118664\n",
      " 0.98489291 0.98716787 0.99223611 0.98343328 0.01025364 0.01123948\n",
      " 0.01297185 0.00659588 0.00916482 0.00905754 0.00946921 0.0135625\n",
      " 0.01544278 0.012199   0.00428843 0.02601686 0.0102537  0.00971894\n",
      " 0.00799461 0.01302621 0.01152049 0.02088694 0.00868    0.01266892\n",
      " 0.00938187 0.0046817  0.01141202 0.02736692 0.02176051 0.00480779\n",
      " 0.01406454 0.00967235 0.01961424 0.00371338 0.01591198 0.01018275\n",
      " 0.01591564 0.0091387  0.00793336 0.01418776 0.01598661 0.00790924\n",
      " 0.00982304 0.01510489 0.0095551  0.00956444 0.01201463 0.00713553\n",
      " 0.01174669 0.01008494 0.00842678 0.01041636 0.0094892  0.00841735\n",
      " 0.01379739 0.01172166 0.00715571 0.01746985 0.0178869  0.01727824\n",
      " 0.01313903 0.01411806 0.01074135 0.00955876 0.00794704 0.00830343\n",
      " 0.01258721 0.01302377 0.0120474  0.01411215 0.01392308 0.00739812\n",
      " 0.02093255 0.00920809 0.00665979 0.00522934 0.01764325 0.00829237\n",
      " 0.01389042 0.02835821 0.01199657 0.0102904  0.01761491 0.0083662\n",
      " 0.01389067 0.00490136 0.00989624 0.00757837 0.01255137 0.00612572\n",
      " 0.00665019 0.00646744 0.0072558  0.00831713 0.01105055 0.0124126\n",
      " 0.00913207 0.01927726 0.01360133 0.01122792 0.00934611 0.00725724\n",
      " 0.00931887 0.00869861]\n"
     ]
    }
   ],
   "source": [
    "# Linear Binary Classification\n",
    "# Given a set of N points in the D-dimensional space and their labels of boolean values, find a hyperplane that separates the true and false class. Return a classifier that takes in a point vector and returns probility.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def binary_classifier_from(X, Y):\n",
    "    '''\n",
    "        X, a float np.array of shape [N, D]\n",
    "        Y, a boolean array of labels, shape [N]\n",
    "    '''\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    D = X.shape[1]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(D)  # D,\n",
    "    b = 0  # 1\n",
    "\n",
    "    lr = 0.001\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return (1 + np.exp(-x)) ** -1\n",
    "    def d_sigmoid(x):\n",
    "        # -1 * (1 + np.exp(-x)) ** -2 * np.exp(-x) * -1\n",
    "        # return sigmoid(x) ** 2 * np.exp(-x)  \n",
    "        return  sigmoid(x) * (1 -  sigmoid(x))\n",
    "    \n",
    "    m = X.mean(0)\n",
    "    std = X.std()\n",
    "\n",
    "    def forward(vectors):\n",
    "        standardized_v = (vectors - m) / (std + epsilon)\n",
    "        logit = standardized_v @ W + b  # N\n",
    "        prob = sigmoid(logit)  # N\n",
    "        return prob\n",
    "\n",
    "    for i in range(200):\n",
    "        # Forward\n",
    "        prob = forward(X)  # N\n",
    "        labels = Y * 1.0\n",
    "        loss = - labels * np.log(prob + epsilon) - (1 - labels) * np.log(1 - prob + epsilon)\n",
    "\n",
    "        # Backward\n",
    "        standardized_x = (X - m) / (std + epsilon)\n",
    "        # d_prob = - labels / (prob + epsilon) + (1 - labels) / (1 - prob + epsilon)  # N\n",
    "        # d_logit = prob * (1 - prob) * d_prob # N\n",
    "        d_logit = prob - labels\n",
    "        d_W = standardized_x.T @ d_logit  # D\n",
    "        d_b = d_logit.sum()\n",
    "\n",
    "        W -= lr * d_W\n",
    "        b -= lr * d_b\n",
    "\n",
    "        print(f\"loss.mean={loss.mean()}\")\n",
    "        # break\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "# train_data = np.array([\n",
    "#     [0, 1], [1, 2], [3, 4], [1, 1], [2, 3],\n",
    "# ], dtype=float)\n",
    "# train_labels = np.array([True, True, True, False, True])\n",
    "\n",
    "# train_data = np.array([\n",
    "#     [0, 1], [1, 2],\n",
    "# ], dtype=float)\n",
    "# train_labels = np.array([True, False])\n",
    "\n",
    "train_data = np.concatenate(\n",
    "    [\n",
    "        np.random.randn(100, 10) + np.array([50, 20] + [0] * 8, dtype=float),\n",
    "        np.random.randn(100, 10) + np.array([20, 0] + [0] * 8, dtype=float),\n",
    "    ], axis=0)\n",
    "train_labels = np.array([True] * 100 + [False] * 100)\n",
    "\n",
    "model = binary_classifier_from(train_data, train_labels)\n",
    "print(model(train_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \n",
    "        d_prob = - labels / (prob + epsilon) + (1 - labels) / (1 - prob + epsilon)  # N\n",
    "        d_logit = prob * (1 - prob) * d_prob # N\n",
    "in the backprob, it's very same:\n",
    "\n",
    "loss.mean=0.47989471852639026\n",
    "loss.mean=0.4190224805366235\n",
    "loss.mean=0.3698198680071503\n",
    "loss.mean=0.32959864629758917\n",
    "loss.mean=0.2963408932624755\n",
    "loss.mean=0.26853386425829784\n",
    "loss.mean=0.24503870297758878\n",
    "loss.mean=0.22499161669233395\n",
    "loss.mean=0.20773139806909896\n",
    "loss.mean=0.1927468346361424\n",
    "loss.mean=0.17963862408216955\n",
    "loss.mean=0.16809170825330783\n",
    "loss.mean=0.1578550528428587\n",
    "loss.mean=0.14872675337547514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_logits=[[  0.4   0.   -4.   -1. ]\n",
      " [  0.4   0.   -4.  -10. ]]\n",
      "softmax(test_logits)=[[5.18351093e-01 3.47461129e-01 6.36397256e-03 1.27823806e-01]\n",
      " [5.94308491e-01 3.98376895e-01 7.29652735e-03 1.80862831e-05]]\n",
      "logits_grad=[[ 0.25917555 -0.32626944  0.00318199  0.0639119 ]\n",
      " [ 0.29715425  0.19918845  0.00364826 -0.49999096]]\n",
      "test_logits_w_d=[[ 4.e-01  0.e+00 -4.e+00 -1.e+00]\n",
      " [ 4.e-01  1.e-04 -4.e+00 -1.e+01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1991944394497125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-class logistic regression\n",
    "\n",
    "# loss = numpy.F.cross_entropy(logits, Y)\n",
    "# what is d_logits?\n",
    "\n",
    "# probs = softmax(logits)\n",
    "# loss = -np.log(probs[y]).mean()\n",
    "\n",
    "# loss = - log(e ** logits[y] / SUM_j(e ** logits[j]) )\n",
    "\n",
    "# dL / d(logits[i]) = - probs[y] ** -1 * d (e ** logits[y] / SUM_j(e ** logits[j]) / d(logits[i])\n",
    "# if i != y\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1  * (-1) e ** logits[y] / SUM_j(e ** logits[j]) ** 2 * e ** logits[i]\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1  * (-1) prob[y] * prob[i]\n",
    "#  dL / d(logits[i]) = prob[i]\n",
    "# if i == y\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 * \n",
    "#      (e ** logits[i] * SUM_j(e ** logits[j]) - d(SUM_j(e ** logits[j]))/dlogit[i] * e ** logits[i])\n",
    "#      / SUM_j(e ** logits[j]) ** 2\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 *\n",
    "#      (e ** logits[i] * SUM_j(e ** logits[j]) - e ** logits[i] * e ** logits[i])\n",
    "#      / SUM_j(e ** logits[j]) ** 2\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 * e ** logits[i] *(SUM_j(e ** logits[j]) -  e ** logits[i])\n",
    "#      / SUM_j(e ** logits[j]) ** 2\n",
    "#  dL / d(logits[i]) = - probs[y] ** -1 * prob[i] *(1 -  e ** logits[i] / SUM_j(e ** logits[j]) )\n",
    "#  dL / d(logits[i]) = - (1 - prob[i])\n",
    "#  dL / d(logits[i]) = prob[i] - 1\n",
    "\n",
    "def softmax(x, dim):\n",
    "  x_maxes = np.max(x, dim, keepdims=True)\n",
    "  norm_x = x - x_maxes # subtract max for numerical stability\n",
    "  counts = np.exp(norm_x)\n",
    "  counts_sum = counts.sum(dim, keepdims=True)\n",
    "  counts_sum_inv = counts_sum**-1\n",
    "  return counts * counts_sum_inv  # B, V\n",
    "\n",
    "def grad_of_loss_wrt_logits(logits, Y):\n",
    "  d_logits = softmax(logits, 1)\n",
    "  d_logits[range(logits.shape[0]), Y] -= 1.0\n",
    "  return d_logits / logits.shape[0]\n",
    "\n",
    "# softmax(np.zeros([2, 2], dtype=float), 1)\n",
    "\n",
    "test_logits = np.array(\n",
    "  [\n",
    "    [0.4, 0.0, -4.0, -1.0],\n",
    "    [0.4, 0.0, -4.0, -10.0],\n",
    "  ],\n",
    "  dtype=float,\n",
    ")\n",
    "# test_logits.shape\n",
    "test_Y = np.array([1, 3], dtype=int)\n",
    "\n",
    "logits_grad = grad_of_loss_wrt_logits(test_logits, test_Y)\n",
    "print(f\"test_logits={test_logits}\")\n",
    "print(f\"softmax(test_logits)={softmax(test_logits, 1)}\")\n",
    "print(f\"logits_grad={logits_grad}\")\n",
    "\n",
    "# verify by numerical gradient\n",
    "epsilon = 0.0001\n",
    "test_logits_w_d = test_logits.copy()\n",
    "test_logits_w_d[1, 1] += epsilon\n",
    "print(f\"test_logits_w_d={test_logits_w_d}\")\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "  '''\n",
    "    prob: dtype float, shape of [B, C], the probability of each class\n",
    "    labels: dtype int, shape of [B, 1], the class label indices\n",
    "  '''\n",
    "  prob = softmax(logits, 1)\n",
    "  return -np.log(prob)[range(logits.shape[0]), labels].mean()\n",
    "\n",
    "(cross_entropy_loss(test_logits_w_d, test_Y) - cross_entropy_loss(test_logits, test_Y)) / epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
