{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFUyH8X3gfYd3GMZW42z4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FengDSP/colabs/blob/main/Tensor_Parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import threading"
      ],
      "metadata": {
        "id": "jQLzbXxauec6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 40\n",
        "D = 20\n",
        "H = 60\n",
        "LR = 0.01\n",
        "EPOCHS = 10\n",
        "\n",
        "labels = np.random.randint(low=0, high=2, size=(N))\n",
        "features = np.random.randn(N, D)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "  return - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "\n",
        "TP = 2\n",
        "# Column parallel -> H -> Row parallel -> logit\n",
        "assert H % TP == 0\n",
        "\n",
        "layer_tp = [np.random.randn(D, H // TP) * 0.1 for _ in range(TP)]\n",
        "bias_tp = [np.zeros(H // TP) for _ in range(TP)]\n",
        "proj_tp = [np.random.randn(H // TP) for _ in range(TP)]\n",
        "p_bias_tp = np.zeros(1)"
      ],
      "metadata": {
        "id": "Ot8h41Mr5WFx"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  layer = np.concatenate(layer_tp, axis=1)\n",
        "  bias = np.concatenate(bias_tp)\n",
        "  proj = np.concatenate(proj_tp)\n",
        "  p_bias = p_bias_tp.copy()\n",
        "\n",
        "  for e in range(EPOCHS):\n",
        "    # forward\n",
        "    x = features\n",
        "    preact = x @ layer + bias\n",
        "    activation = np.maximum(0, preact)  # N, H\n",
        "    logits = activation @ proj + p_bias\n",
        "    prob = sigmoid(logits)\n",
        "\n",
        "    # loss\n",
        "    loss = cross_entropy(prob, labels).mean()\n",
        "    print(f\"Epoch {e} loss={loss}\")\n",
        "\n",
        "    # backward\n",
        "    d_logit = prob - labels  # N\n",
        "    d_p_bias = d_logit.sum()\n",
        "    d_proj = activation.T @ d_logit   # H, 1\n",
        "    d_activation = d_logit.reshape(-1, 1) @ proj.reshape(1, -1)    # N, H\n",
        "    d_preact = d_activation * (preact > 0)  # N, H\n",
        "    d_bias = d_preact.sum(axis=0)  # H\n",
        "    d_layer = x.T @ d_preact  # D, H\n",
        "\n",
        "    layer -= LR * d_layer\n",
        "    bias -= LR * d_bias\n",
        "    proj -= LR * d_proj\n",
        "    p_bias -= LR * d_p_bias\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPBRqzBCNV76",
        "outputId": "d4223f82-bafb-4b55-e678-ab4e120452f8"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss=0.9487139530194348\n",
            "Epoch 1 loss=0.44234263503102617\n",
            "Epoch 2 loss=0.1752792993882155\n",
            "Epoch 3 loss=0.09966124766672273\n",
            "Epoch 4 loss=0.07375039541089778\n",
            "Epoch 5 loss=0.06169647865232828\n",
            "Epoch 6 loss=0.053369447126086336\n",
            "Epoch 7 loss=0.047316068306977585\n",
            "Epoch 8 loss=0.042476203622106634\n",
            "Epoch 9 loss=0.03858467879545534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Comm(object):\n",
        "  def __init__(self):\n",
        "    self.value = None\n",
        "    self.reduce_barrier = threading.Barrier(TP)\n",
        "    self.reset_barrier = threading.Barrier(TP)\n",
        "    self.return_barrier = threading.Barrier(TP)\n",
        "    self.lock = threading.Lock()\n",
        "\n",
        "  def all_reduce(self, x, op=None):\n",
        "    assert op == 'sum', f'Not supporting {op} yet'\n",
        "    assert x is not None\n",
        "    with self.lock:\n",
        "      if self.value is None:\n",
        "        self.value = x\n",
        "      else:\n",
        "        self.value += x\n",
        "    self.reduce_barrier.wait()\n",
        "    result = self.value\n",
        "    if self.reset_barrier.wait() == 0:\n",
        "      self.value = None\n",
        "    self.return_barrier.wait()\n",
        "    return result"
      ],
      "metadata": {
        "id": "jiBDZU_Q98b-"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "qILorJP2uHE9"
      },
      "outputs": [],
      "source": [
        "def train_worker(i, comm):\n",
        "  layer = layer_tp[i].copy()\n",
        "  bias = bias_tp[i].copy()\n",
        "  proj = proj_tp[i].copy()\n",
        "  p_bias = p_bias_tp.copy()\n",
        "  for e in range(EPOCHS):\n",
        "    # forward\n",
        "    x = features\n",
        "    preact = x @ layer + bias\n",
        "    activation = np.maximum(0, preact)  # N, H\n",
        "    logits = activation @ proj\n",
        "    if i == 0:\n",
        "      logits += p_bias\n",
        "    logits = comm.all_reduce(logits, op='sum')\n",
        "    if i == 0:\n",
        "      prob = sigmoid(logits)\n",
        "      # loss\n",
        "      loss = cross_entropy(prob, labels).mean()\n",
        "      print(f\"Epoch {e} loss={loss}\")\n",
        "\n",
        "      # backward\n",
        "      d_logit = prob - labels  # N\n",
        "      d_p_bias = d_logit.sum()\n",
        "    else:\n",
        "      d_logit = np.zeros_like(logits)\n",
        "    d_logit = comm.all_reduce(d_logit, op='sum')\n",
        "    d_proj = activation.T @ d_logit   # H, 1\n",
        "    d_activation = d_logit.reshape(-1, 1) @ proj.reshape(1, -1)    # N, H\n",
        "    d_preact = d_activation * (preact > 0)  # N, H\n",
        "    d_bias = d_preact.sum(axis=0)  # H\n",
        "    d_layer = x.T @ d_preact  # D, H\n",
        "\n",
        "    layer -= LR * d_layer\n",
        "    bias -= LR * d_bias\n",
        "    proj -= LR * d_proj\n",
        "    if i == 0:\n",
        "      p_bias -= LR * d_p_bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threads = []\n",
        "comm = Comm()\n",
        "for i in range(TP):\n",
        "  t = threading.Thread(target=train_worker, args=(i, comm))\n",
        "  t.start()\n",
        "  threads.append(t)\n",
        "\n",
        "for t in threads:\n",
        "  t.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4PfAxyC6esU",
        "outputId": "e3895b2c-4dbf-46fc-f61e-bdd9d4a49f50"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss=0.9487139530194348\n",
            "Epoch 1 loss=0.44234263503102617\n",
            "Epoch 2 loss=0.17527929938821551\n",
            "Epoch 3 loss=0.09966124766672273\n",
            "Epoch 4 loss=0.0737503954108978\n",
            "Epoch 5 loss=0.06169647865232828\n",
            "Epoch 6 loss=0.05336944712608632\n",
            "Epoch 7 loss=0.0473160683069776\n",
            "Epoch 8 loss=0.042476203622106634\n",
            "Epoch 9 loss=0.03858467879545534\n"
          ]
        }
      ]
    }
  ]
}