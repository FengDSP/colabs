{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzBeUQRwvR8cxU88dpN9Bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FengDSP/colabs/blob/main/Tensor_Parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import threading"
      ],
      "metadata": {
        "id": "jQLzbXxauec6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "D = 20\n",
        "H = 40\n",
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "\n",
        "labels = np.random.randint(low=0, high=2, size=(N))\n",
        "features = np.random.randn(N, D)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "  return - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "\n",
        "TP = 2\n",
        "# Column parallel -> H -> Row parallel -> logit\n",
        "assert H % TP == 0\n",
        "\n",
        "layer_tp = [np.random.randn(D, H // TP) * 0.1 for _ in range(TP)]\n",
        "bias_tp = [np.zeros(H // TP) for _ in range(TP)]\n",
        "proj_tp = [np.random.randn(H // TP) for _ in range(TP)]\n",
        "p_bias_tp = np.zeros(1)"
      ],
      "metadata": {
        "id": "Ot8h41Mr5WFx"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  layer = np.concatenate(layer_tp, axis=1)\n",
        "  bias = np.concatenate(bias_tp)\n",
        "  proj = np.concatenate(proj_tp)\n",
        "  p_bias = p_bias_tp.copy()\n",
        "\n",
        "  for e in range(EPOCHS):\n",
        "    # forward\n",
        "    x = features\n",
        "    preact = x @ layer + bias\n",
        "    activation = np.maximum(0, preact)  # N, H\n",
        "    logits = activation @ proj + p_bias\n",
        "    prob = sigmoid(logits)\n",
        "\n",
        "    # loss\n",
        "    loss = cross_entropy(prob, labels).mean()\n",
        "    print(f\"Epoch {e} loss={loss}\")\n",
        "\n",
        "    # backward\n",
        "    d_logit = prob - labels  # N\n",
        "    d_p_bias = d_logit.sum()\n",
        "    d_proj = activation.T @ d_logit   # H, 1\n",
        "    d_activation = d_logit.reshape(-1, 1) @ proj.reshape(1, -1)    # N, H\n",
        "    d_preact = d_activation * (preact > 0)  # N, H\n",
        "    d_bias = d_preact.sum(axis=0)  # H\n",
        "    d_layer = x.T @ d_preact  # D, H\n",
        "\n",
        "    layer -= LR * d_layer\n",
        "    bias -= LR * d_bias\n",
        "    proj -= LR * d_proj\n",
        "    p_bias -= LR * d_p_bias\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPBRqzBCNV76",
        "outputId": "b78ca94d-419b-44d5-d497-295d82d9c4d2"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss=0.6685087322760365\n",
            "Epoch 1 loss=0.5463913051939092\n",
            "Epoch 2 loss=0.44949604756574846\n",
            "Epoch 3 loss=0.3747345190150955\n",
            "Epoch 4 loss=0.3183985839512868\n",
            "Epoch 5 loss=0.27618571016018956\n",
            "Epoch 6 loss=0.24490588669293967\n",
            "Epoch 7 loss=0.21992188152893805\n",
            "Epoch 8 loss=0.19964563485403025\n",
            "Epoch 9 loss=0.18278670993041032\n",
            "Epoch 10 loss=0.1688279026633367\n",
            "Epoch 11 loss=0.15672002634955073\n",
            "Epoch 12 loss=0.14628174447523565\n",
            "Epoch 13 loss=0.13721570448804848\n",
            "Epoch 14 loss=0.12927649340361408\n",
            "Epoch 15 loss=0.12218339901263808\n",
            "Epoch 16 loss=0.1158466689141086\n",
            "Epoch 17 loss=0.11021568026407172\n",
            "Epoch 18 loss=0.10524478165190343\n",
            "Epoch 19 loss=0.10067106574464216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "barrier = threading.Barrier(TP)\n",
        "to_reduce = []\n",
        "lock = threading.Lock()\n",
        "\n",
        "def all_reduce(x, op=None):\n",
        "  assert op == 'sum'\n",
        "  with lock:\n",
        "    if not to_reduce:\n",
        "      to_reduce.append(x)\n",
        "    else:\n",
        "      to_reduce[0] += x\n",
        "  barrier.wait()\n",
        "  result = to_reduce[0]\n",
        "  if barrier.wait() == 0:\n",
        "    to_reduce.clear()\n",
        "  return result"
      ],
      "metadata": {
        "id": "jiBDZU_Q98b-"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "qILorJP2uHE9"
      },
      "outputs": [],
      "source": [
        "def train_worker(i):\n",
        "  layer = layer_tp[i].copy()\n",
        "  bias = bias_tp[i].copy()\n",
        "  proj = proj_tp[i].copy()\n",
        "  p_bias = p_bias_tp.copy()\n",
        "  for e in range(EPOCHS):\n",
        "    # forward\n",
        "    x = features\n",
        "    preact = x @ layer + bias\n",
        "    activation = np.maximum(0, preact)  # N, H\n",
        "    logits = activation @ proj\n",
        "    if i == 0:\n",
        "      logits += p_bias\n",
        "    logits = all_reduce(logits, op='sum')\n",
        "    if i == 0:\n",
        "      prob = sigmoid(logits)\n",
        "      # loss\n",
        "      loss = cross_entropy(prob, labels).mean()\n",
        "      print(f\"Epoch {e} loss={loss}\")\n",
        "\n",
        "      # backward\n",
        "      d_logit = prob - labels  # N\n",
        "      # print(f'd_logit={d_logit}')\n",
        "      d_p_bias = d_logit.sum()\n",
        "    else:\n",
        "      d_logit = np.zeros_like(logits)\n",
        "    d_logit = all_reduce(d_logit, op='sum')\n",
        "    # print(f'd_logit={d_logit} after all readuce')\n",
        "    d_proj = activation.T @ d_logit   # H, 1\n",
        "    d_activation = d_logit.reshape(-1, 1) @ proj.reshape(1, -1)    # N, H\n",
        "    d_preact = d_activation * (preact > 0)  # N, H\n",
        "    d_bias = d_preact.sum(axis=0)  # H\n",
        "    d_layer = x.T @ d_preact  # D, H\n",
        "\n",
        "    layer -= LR * d_layer\n",
        "    bias -= LR * d_bias\n",
        "    proj -= LR * d_proj\n",
        "    if i == 0:\n",
        "      p_bias -= LR * d_p_bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threads = []\n",
        "for i in range(TP):\n",
        "  t = threading.Thread(target=train_worker, args=(i, ))\n",
        "  t.start()\n",
        "  threads.append(t)\n",
        "\n",
        "for t in threads:\n",
        "  t.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4PfAxyC6esU",
        "outputId": "1dc1b9e7-079e-4701-97a2-53b94943a850"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss=4.912591801666268\n",
            "Epoch 1 loss=2.7430450759191065\n",
            "Epoch 2 loss=1.5102569861675312\n",
            "Epoch 3 loss=1.278795563554971\n",
            "Epoch 4 loss=1.6803453768683796\n",
            "Epoch 5 loss=2.905304303601011\n",
            "Epoch 6 loss=3.309683561604799\n",
            "Epoch 7 loss=3.8384999270550253\n",
            "Epoch 8 loss=3.643791742226925\n",
            "Epoch 9 loss=3.2039173561849914\n",
            "Epoch 10 loss=3.270571429564888\n",
            "Epoch 11 loss=3.654928254525329\n",
            "Epoch 12 loss=4.448823261521026\n",
            "Epoch 13 loss=5.068537052084682\n",
            "Epoch 14 loss=4.5439236683669915\n",
            "Epoch 15 loss=4.039457339368953\n",
            "Epoch 16 loss=3.4477388872852033\n",
            "Epoch 17 loss=2.9010270219900334\n",
            "Epoch 18 loss=2.633577601868524\n",
            "Epoch 19 loss=2.9158517711979997\n"
          ]
        }
      ]
    }
  ]
}